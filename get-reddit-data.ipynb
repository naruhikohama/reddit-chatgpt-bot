{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yaml\") as f:\n",
    "    config = yaml.load(f, SafeLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a read-only Reddit instance\n",
    "reddit = praw.Reddit(client_id = config['reddit']['client_id'],\n",
    "                     client_secret = config['reddit']['client_secret'],\n",
    "                     redirect_url = 'http://localhost:8080',\n",
    "                     user_agent = config['reddit']['user_agent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_posts(subreddit_list = \"MachineLearning\", limit = 1000, time_filter = 'all'):\n",
    "    reddit = praw.Reddit(client_id = config['reddit']['client_id'],\n",
    "                         client_secret = config['reddit']['client_secret'],\n",
    "                         redirect_url = 'http://localhost:8080',\n",
    "                         user_agent = config['reddit']['user_agent']\n",
    "                         )\n",
    "\n",
    "    posts = reddit.subreddit(subreddit_list).top(time_filter=time_filter, limit=limit)\n",
    "\n",
    "    # Initialize dataframe\n",
    "    posts_df = []\n",
    "\n",
    "    for post in posts:\n",
    "        posts_df.append({'post_id': post.id,\n",
    "                        'subreddit': post.subreddit,\n",
    "                        'created_utc': post.created_utc,\n",
    "                        'selftext': post.selftext,\n",
    "                        'post_url': post.url,\n",
    "                        'post_title':post.title,\n",
    "                        'link_flair_text': post.link_flair_text,\n",
    "                        'score': post.score,\n",
    "                        'num_comments': post.num_comments,\n",
    "                        'upvote_ratio': post.upvote_ratio\n",
    "                        })\n",
    "        \n",
    "    return pd.DataFrame(posts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df = get_top_posts(\"MachineLearning+artificial+datascience\", limit=1500, time_filter='all')\n",
    "posts_df.to_csv(\"data/ds_ml_ai_reddit_posts.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>selftext</th>\n",
       "      <th>post_url</th>\n",
       "      <th>post_title</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>upvote_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gh1dj9</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>1.589117e+09</td>\n",
       "      <td></td>\n",
       "      <td>https://v.redd.it/v492uoheuxx41</td>\n",
       "      <td>[Project] From books to presentations in 10s w...</td>\n",
       "      <td>Project</td>\n",
       "      <td>7791</td>\n",
       "      <td>186</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kuc6tz</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>1.610275e+09</td>\n",
       "      <td></td>\n",
       "      <td>https://v.redd.it/25nxi9ojfha61</td>\n",
       "      <td>[D] A Demo from 1993 of 32-year-old Yann LeCun...</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>5852</td>\n",
       "      <td>133</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>g7nfvb</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>1.587789e+09</td>\n",
       "      <td></td>\n",
       "      <td>https://v.redd.it/rlmmjm1q5wu41</td>\n",
       "      <td>[R] First Order Motion Model applied to animat...</td>\n",
       "      <td>Research</td>\n",
       "      <td>4752</td>\n",
       "      <td>111</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lui92h</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>1.614525e+09</td>\n",
       "      <td></td>\n",
       "      <td>https://v.redd.it/ikd5gjlbi8k61</td>\n",
       "      <td>[N] AI can turn old photos into moving Images ...</td>\n",
       "      <td>News</td>\n",
       "      <td>4687</td>\n",
       "      <td>230</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ohxnts</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>1.625977e+09</td>\n",
       "      <td></td>\n",
       "      <td>https://i.redd.it/34sgziebfia71.jpg</td>\n",
       "      <td>[D] This AI reveals how much time politicians ...</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>4571</td>\n",
       "      <td>228</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  post_id        subreddit   created_utc selftext  \\\n",
       "0  gh1dj9  MachineLearning  1.589117e+09            \n",
       "1  kuc6tz  MachineLearning  1.610275e+09            \n",
       "2  g7nfvb  MachineLearning  1.587789e+09            \n",
       "3  lui92h  MachineLearning  1.614525e+09            \n",
       "4  ohxnts  MachineLearning  1.625977e+09            \n",
       "\n",
       "                              post_url  \\\n",
       "0      https://v.redd.it/v492uoheuxx41   \n",
       "1      https://v.redd.it/25nxi9ojfha61   \n",
       "2      https://v.redd.it/rlmmjm1q5wu41   \n",
       "3      https://v.redd.it/ikd5gjlbi8k61   \n",
       "4  https://i.redd.it/34sgziebfia71.jpg   \n",
       "\n",
       "                                          post_title link_flair_text  score  \\\n",
       "0  [Project] From books to presentations in 10s w...         Project   7791   \n",
       "1  [D] A Demo from 1993 of 32-year-old Yann LeCun...      Discussion   5852   \n",
       "2  [R] First Order Motion Model applied to animat...        Research   4752   \n",
       "3  [N] AI can turn old photos into moving Images ...            News   4687   \n",
       "4  [D] This AI reveals how much time politicians ...      Discussion   4571   \n",
       "\n",
       "   num_comments  upvote_ratio  \n",
       "0           186          0.99  \n",
       "1           133          0.98  \n",
       "2           111          0.97  \n",
       "3           230          0.97  \n",
       "4           228          0.96  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fact that they also had to know the location of the numbers and that the algorithm was robust to scale changes is impressive for 1993\n",
      "\n",
      "It's not like they just solved MNIST in 1993, it's one step above that\n",
      "Every data scientist today is truly standing on the shoulders of giants.\n",
      "awesome to see\n",
      "TIL audio hasnâ€™t been invented until 1994\n",
      "And yet websites still think those obfuscated texts are a good test for robots\n",
      "Anyone know who the other guys at the end are?\n",
      "Man, these guys were the real engineers.\n",
      "Actually, he was 32 years old when he pressed the button. He was 33 by the time he got the results back.\n",
      "Never going to complain about not having a strong enough GPU again. Very cool.\n",
      "Wonder what was the RAM and computing power of the system.\n",
      "Many donâ€™t know it, but before it was done such text recognition was considered impossible, just like AGI and other hard problems. I think text recognition in mail was the first successful real world application of AI.\n",
      "MNIST irl\n",
      "that was certainly more wholesome than the other historic computer vision video, [https://www.youtube.com/watch?v=8VdFf3egwfg](https://www.youtube.com/watch?v=8VdFf3egwfg)\n",
      "But the question is: is it the validation set? ðŸ˜\n",
      "Very inspiring as I remember these days. Lot of hard work and at the cutting edge.\n",
      "Uh.  Sorry, no.\n",
      "\n",
      "[The CNN was invented by Hubel and Weisel in 1959, the year before Yann LeCun was born, under the name \"neocognitron.\"](https://en.wikipedia.org/wiki/Neocognitron) \n",
      "\n",
      "LeCun also didn't make them first.\n",
      "\n",
      "[The CNN was first implemented by Kunihiko Fukushima in 1979](https://search.ieice.org/bin/summary.php?id=j62-a_10_658), 14 years before this video\n",
      "\n",
      "(Reference translated is Journal of the Institute of Electronics, Information and Communication Engineers A Vol.J62-A No.10 pp.658-665, October 25, 1979, ISSN 0373-6091)\n",
      "\n",
      "What Yann LeCun actually brought to the party was the modern approach to training them.  He did that in 1984, not 1993.\n",
      "[deleted]\n",
      "Nice keeb.\n",
      "u/savevideo\n",
      "That is so satisfying\n",
      "The first set of numbers was Yann LeCun's phone number at bell labs.\n",
      "Still accurate than tesseract lol ðŸ˜‚\n",
      "So why am I still doing captchas\n",
      "Yann LeCun's tweet on who the other guys are, and who the cameraman is - \n",
      "https://twitter.com/ylecun/status/1347268914263306242?s=20\n",
      "Better than tesseract\n",
      "But still, to this date, they cannot recognize traffic lights\n",
      "incredible! pay tribute to him\n",
      "So why did it take 30 years to get this far?\n",
      "On some comments about possible tweaks/tricks in this video:\n",
      "\n",
      "I have had the privilege to attend professor Yann's classes at NYU. \n",
      "From whatever little I understand of him - he has high levels of integrity, and I do not see him trying some cheap tweaks and fixes...He was committed to solve a problem in the best way possible and not just for likes and hearts â˜ºï¸.\n",
      "\n",
      "And without high level of integrity, you can't go from lab to national level in short time. \n",
      " \n",
      "People often underestimate what it takes to be unanimously accepted as one of the godfathers of current hottest trend. This doesn't discount the effort of forefathers or future generations...\n",
      "... but let's not undermine Prof's integrity and commitment by making such frivolous comments. In fact, it is only our loss, if we fail to see that.\n",
      "Cant see his right hand\n",
      "Outside of the CNN achievements the rest is actually impressive too, and I'm absolutely amazed that the interface is so responsive. In 1993.\n",
      "I'll never understand why this didn't blow up like it should have when they succeeded in doing this. Should've been in the news all over the place for months.\n",
      "\n",
      "AI winter my backside\n",
      "What a boss!\n",
      "Fukushimaâ€™s neocognitron came almost two decades earlier.\n",
      "So then what took so long for it to catch on? Why did it take another 30 years if they knew the power of cnn's?\n",
      "Amazing!  Iâ€™ve cited Professor LeCunn multiple times and am always humbled by his work â€” this is why I tell students that they are standing on the shoulders of giants when they do research.  Love this video!!!\n",
      "Are you sure youâ€™re a robot?\n",
      "WOW !!! Impressive !\n",
      "Where was the video shot?\n",
      "I guess too many people underestimate what could be accomplished with a little and tons of passion and time\n",
      "Agree - it was 6 years later until MNIST was even released.\n",
      "I guess they had a preprocessing step to identify, center and scale each digit image before feeding into the neural network. Itâ€™s not that hard with feature engineering.\n",
      "The video has lots of cuts, and the numbers never obscures an important part of the image...    I suspect each of those tests had tweaking and tuning to make it work...\n",
      "Love how happy they look!\n",
      "I was born 1982. We didn't start hearing shit until 1995. That was an absolutely wild year. It created a real musical renaissance.\n",
      "Can confirm.  That's the year I got a sound card.\n",
      "there is a reason why captcha is becoming obsolete. At least the text based version.\n",
      "\n",
      "Also, captcha actually digitize books. This is why there are 2 tests, not 1. So in a sense, we were training the robots filling the captchas.\n",
      "I donâ€™t think itâ€™s meant to filter that way. Bots usually are built with speed in mind so it recognises and fills in the blanks virtually immediately.\n",
      "\n",
      "That and captchas are also useful for labelling training datasets manually (user input). But correct me if Iâ€™m wrong though.\n",
      "It serves two purposes. It defeats 99.99% of bots, and it maps images to human inputs to train their image recognizer networks.\n",
      "Unless someone cares enough about your little website to train an AI to solve your captcha they're still not a terrible idea. I don't think there are any AIs that are generic enough to solve *all* obfuscated text captchas yet.\n",
      "\n",
      "Obviously it's not going to work for large sites but none of them use that method anymore anyway.\n",
      "Am son of the guy in the chair (Rich Howard, collaborator and director of the silicon integrated circuit lab at the time). He said the guy in orange was a technician and computer whiz named Donnie Henderson.\n",
      "err, Kurzweil had an OCR product in 1976: [https://en.wikipedia.org/wiki/Ray\\_Kurzweil#Mid-life](https://en.wikipedia.org/wiki/Ray_Kurzweil#Mid-life)\n",
      "Bayesian classifiers as the first email spam filter?\n",
      "\n",
      "Not sure the year, but our lives would be completely different if it wasnâ€™t for it.\n",
      "> Many donâ€™t know it, but before it was done such text recognition was considered impossible\n",
      "\n",
      "By the time LeCun did this, text recognition was common at banks for scanning checks, in children's toys, and was the basis of the Cue:CAT.\n",
      "\n",
      "You're making this up.\n",
      "\n",
      "OCR was common by the early 1970s, almost 30 years before this.\n",
      "No\n",
      "You are NOT correct about Hubel and Weisel.\n",
      "\n",
      "Hubel and Weisel did research on visual cortex in real brains (in cats) and it was awesome (they got Nobel Prize for it). But they did not invent CNNs.\n",
      "\n",
      "You can read their paper \\[1\\] you don't have to be a biologists to understand most of it. From their work one can deduce what neurons in V1 do. It was later even verified that some of these neurons realize functions similar to Gabor filters, but (as I remember) that was even later then neocognitron.\n",
      "\n",
      "It is true that their findings did *inspire* creators of neocognitron \\[2\\] but that's about it.\n",
      "\n",
      "\\[1\\] [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1363130/pdf/jphysiol01298-0128.pdf](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1363130/pdf/jphysiol01298-0128.pdf)\n",
      "\n",
      "\\[2\\] Fukushima, Kunihiko, and Sei Miyake. \"Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition.\" *Competition and cooperation in neural nets*. Springer, Berlin, Heidelberg, 1982. 267-285.\n",
      "And to add to this, people thought NN's were a joke until a CNN won an image recognition contest in 2012, which is what put them on the map.  Before that they were obscure and overlooked.\n",
      "Hubel and Wiesel, building upon the work of Vernon Mountcastle, analyzed the structure and organization of neurons in the visual cortex of cats.\n",
      "\n",
      "Fukushima did not use convolutional layers or convolutional operations for the neocognitron, therefore it does not fit the description of convolutional neural network.\n",
      "\n",
      "It does fit the description of deep learning though.\n",
      "at least, read the title\n",
      "###[View link](https://redditsave.com/info?url=/r/MachineLearning/comments/kuc6tz/d_a_demo_from_1993_of_32yearold_yann_lecun/)\n",
      "\n",
      "\n",
      " --- \n",
      " [**Info**](https://np.reddit.com/user/SaveVideo/comments/jv323v/info/)&#32;|&#32; [**Feedback**](https://np.reddit.com/message/compose/?to=Kryptonh&subject=Feedback for savevideo) &#32;|&#32; [**DMCA**](https://np.reddit.com/message/compose/?to=Kryptonh&subject=Content removal request for savevideo)&#32;|&#32;[**Donate**](https://ko-fi.com/getvideo)\n",
      "Yes. Back then, the proportion of developers who could hand-write a new graphics algorithm in assembler or C was considerably higher, since that was often how it was done anyway. Necessity is a great motivator. The non-ML part of this problem is more tedious than difficult.\n",
      "Maybe Iâ€™m misunderstanding, but isnâ€™t the whole point of CNNs that the location of the digits doesnâ€™t matter?\n",
      "This system ended up deployed in banks to parse written checks, so I don't think it was tweaked just for these examples, but they did expect to have fully visible digits.\n",
      "I have had the privilege to attend professor Yann's classes at NYU. \n",
      "From whatever little I understand of him - he has high levels of integrity, and I do not see him trying some cheap tweaks and fixes...He was committed to solve a problem in the best way possible and not just for likes and hearts â˜ºï¸.\n",
      "\n",
      "And without that level of integrity, you can't go from lab to national level in short time.\n",
      "[removed]\n",
      "I remember when we got sound in school for the first time there was alot of realization of where smells were actually coming from that day\n",
      "Imagine if The Bends was the first sound you ever heard.\n",
      "Yeah, well I was born in 72 and we ate rocks for breakfast!\n",
      "Lies and slander, PC speaker was readily available on PC before soundcards became a thing.\n",
      "Yeah, on a PHPBB forum I manage, the bots can get through the text-based captchas very easily. But they still struggle with simple questions like \"In what State is this club based?\"\n",
      "Not anymore. Google stopped doing that a while ago\n",
      "Youâ€™re definitely correct about the captchas.  \n",
      "Itâ€™s no coincidence that most of the objects they ask you to recognize are cars, crosswalks etc.  \n",
      "They basically get free labor to help them build a giant dataset fir training self driving cars.\n",
      "what would be the problem with a little delay?\n",
      "As I make my living making bots and doing automation, captcha is just part of the job. Solving captcha isnâ€™t a special thing.\n",
      "â€œIâ€™m not a robotâ€ - select crosswalk, identify license plates, etc. are for training self driving vehicles and finding the house address was for google maps. \n",
      "\n",
      "We should be paid for doing reCaptchas. However some people actually do get paid for these tasks.\n",
      "That's super cool lol. Did this invention have a big impact on their career?\n",
      "Correct me if I'm wrong, but doesn't normal font imply a \"set font\" rather than handwritten characters?\n",
      "\n",
      "Still impressive but a different problem from MNIST and generally reading the messy writing of humans.\n",
      "But Schmidhuber had already written the paper in 1962\n",
      "> Bayesian classifiers as the first email spam filter?\n",
      "\n",
      "You're off by about 9 years.  Bayesian classifiers didn't emerge as spam filters until approximately 1996.  They are currently believed to be first published by [Sahami et al in 1998](http://robotics.stanford.edu/users/sahami/papers-dir/spam.pdf).  That paper describes secretly internally using the technique in late 1996, and is the earliest known published discussion.  The internet at large caught on in 1999, just 22 years ago.\n",
      "\n",
      "The word SPAM actually comes from IRC and MUDs; we had spam filters long, long before email had spam, thanks to terminal washes and things of that nature.  The earliest known IRC spam filter was the `anarchy eris.berkeley.edu` stripper, which didn't work well enough, and led to the split of Jakko's original network to create eris-free net (EFnet is fundamentally named for a spam host removal.)\n",
      "\n",
      "If you count the invention of the q-line as an anti-spam strategy, then IRC invents spam filtering in 1991.  If you require message or origin testing, IRC invents it in 1992 instead.\n",
      "\n",
      "If you're old enough, you remember when Bayesian Filtering turned spam filtering from an ongoing joke into something that actually worked.  This was one of `gmail`'s early advantages.\n",
      "cuecat was a barcode scanner. Never did anything resembling text recognition. Nor were there any children's toys in the 90s or before that did anything of the sort (though they might do interesting stuff to convince *children* that they could!). And check recognition worked by \"cheating\" â€” first, using a special typeface with super easily distinguished characters and uniform size and spacing, and second, [printing it with magnetic ink](https://en.wikipedia.org/wiki/Magnetic_ink_character_recognition) so that the scanner didn't have to find the data it wanted among any kind of visual background. Everything except the routing and account numbers was invisible to it.\n",
      "> It is true that their findings did inspire creators of neocognitron [2] but that's about it.\n",
      "\n",
      "Uh, no, they're where that name comes from.\n",
      "\n",
      "What specific difference do you imagine exists between the neocognitron and CNNs?  They're both striding convolutions as a reduction for inputs.\n",
      "I'm not sure why you believe this.  Neural networks have been a big deal since the 1950s, taking down investments of half a billion at a time from the military for 70+ years now.\n",
      "Has this changed really ? :) In number of engineers with these skills, certainly, in proportion of developers, this remains to be seen. Python is the syntactic sugar but who goes really in and looks under the rug ?\n",
      "The assm skill was crazy back in the day! Nowadays I wouldn't use assm even with an 8bit microcontroller because I'm too lazy.\n",
      "Today's software are thousands times less efficient, because of all the overhead have been added layers on top of layes don't do any real work. Think about after all the closest, cabinets, drawers, boxes, organizers and wrappers, you still get the same pair of old socks and everyone cheers: \"Yeah! It works! We got the socks!\", that's what modern software actually is. But thank to these overhead, this industry have enough investment to support millions of overpaid software engineers, and most important of all, thousands of billionaires.\n",
      "CNN is robust to translation but not invariant to scale and rotation. Max pooling can be used to to combine detectors that trained for different scales and rotations.\n",
      "Did LeCunn make a lot of money from it?\n",
      "I don't doubt that his approach works, or his scientific integrity - simply that for each demo he might have loaded a different model for example (trained for different sizes or handwritten/typed text).\n",
      "This thread feels like r/KenM material\n",
      "At least you were born after color was invented, back in '53.\n",
      "I would struggle too\n",
      "Or \"What is god\"?\n",
      "I wish I could opt out. I donâ€™t want to train skynet lol\n",
      "It greatly reduced the rate at which a bot can do whatever. With no delay something like filling out a form could probably be done thousands of times a second, but if you introduce a 0.1s delay by requiring some model to run then suddenly the maximum rate you can automatically fill out the same form is 10 times a second.\n",
      "\n",
      "Additionally, any more hurdles will naturally mean people need to be more sophisticated to get past them and you'll filter out a lot of the lowest effort bots.\n",
      "Also running a model involves computing costs\n",
      "That sounds fun. You have a site or a blog?\n",
      "Rich was already close to retirement at the time, so not really. Not sure about Donnie.\n",
      "Yann LeCun got the turing award for it\n",
      "In the wiki page (I put it at the right chapter) they state it was supposed to be \"omni-font\" as in reading all types of text, while *older* systems only recognized some set fonts. Note that there were already functional devices. Of course, those probably were of much worse quality than LeCun's small CNN, I just wanted to point out the person I'm responding to is full of shit.\n",
      "> Correct me if I'm wrong, but doesn't normal font imply a \"set font\"\n",
      "\n",
      "1. You're wrong\n",
      "1. Kurtzweil didn't invent this either\n",
      "1. The work being discussed here, the CNN, is actually from the late 1950s, from before LeCun was born\n",
      "**[Magnetic ink character recognition](https://en.wikipedia.org/wiki/Magnetic ink character recognition)**\n",
      "\n",
      "Magnetic ink character recognition code, known in short as MICR code, is a character recognition technology used mainly by the banking industry to streamline the processing and clearance of cheques and other documents. MICR encoding, called the MICR line, is at the bottom of cheques and other vouchers and typically includes the document-type indicator, bank code, bank account number, cheque number, cheque amount (usually added after a cheque is presented for payment), and a control indicator. The format for the bank code and bank account number is country-specific. The technology allows MICR readers to scan and read the information directly into a data-collection device.\n",
      "\n",
      "[^(About Me)](https://np.reddit.com/user/wikipedia_text_bot/comments/jrn2mj/about_me/) ^- [^(Opt out)](https://np.reddit.com/user/wikipedia_text_bot/comments/jrti43/opt_out_here/) ^(- OP can reply !delete to delete) ^- [^(Article of the day)](https://np.reddit.com/comments/k9hx22)\n",
      "\n",
      "**This bot will soon be transitioning to an opt-in system. Click [here](https://np.reddit.com/user/wikipedia_text_bot/comments/ka4icp/opt_in_for_the_new_system/) to learn more and opt in. Moderators: [click here](https://np.reddit.com/user/wikipedia_text_bot/comments/ka4icp/opt_in_for_the_new_system/) to opt in a subreddit.**\n",
      "NNs have definitely had a ton of research, so I agree that they weren't overlooked. However, up until 2012 they weren't very useful for most applications. Throughout the 2000s, SVMs and tree-based models (like random forests) were SOTA for most tasks. So most researchers put their focus there. \n",
      "\n",
      "2012 marked a transition though, as we then had the hardware support to efficiently train much larger models. This allowed NNs to become SOTA in many tasks and thus the explosion in interest\n",
      "I learned it here: https://youtu.be/uXt8qF2Zzfo\n",
      "In terms of what I intended to say, it's changed a lot. It wasn't an obvious career intially, so it caught a lot of people with a passion for it. The normal path for anyone who wanted visual output or realtime performance was to learn C and assembly. Operating systems were permissive, and memory mapping for access to video memory was either straightforward or documented well enough. Being able to do such things came with the job.. and if someone couldn't do it, that'd disqualify from a big chunk of the industry.\n",
      "\n",
      "I think you may have been referring to necessity being a great motivator.. and its converse -- that lack of necessity is a great blocker. Yep, I would agree. Lots of people in ML would now struggle somewhat with these basic graphical operations, even though the preparatory learning and experience required for it is now much less.\n",
      "I try to do and it is not pretty. Years of toil to make that one layer of cnn faster by inventing new winograd based algorithms. Working on the models are always more recognized.\n",
      "I think that's really cynical. Memory safe languages are a gigantic benefit to society in terms of security and stability.\n",
      "\n",
      "Such inefficiencies being permissible has allowed technology to flourish; a lot of programs would never have been written without being wasteful, see VS code vs Vim or Slack over IRC. IRC and Vim are nice cannot be mainstream and the only editor respectively. I don't see online web apps existing like Google Docs if everything had to be native speed fast. I've seen multiple homeless people with a card reader selling magazines, that's how cheap software has got over time that even homeless people have contactless.\n",
      "\n",
      "Arguably the progression of technology isn't what I'd have wanted to see but it isn't all bad. You can't help but wonder why something is slow on your 4GHz multicore CPU at times though haha.\n",
      "No, he was an employee at Bell Labs, the product and patents belonged to Bell Labs.\n",
      "\n",
      "When AT&T spun off Lucent in 1996, the patents went that way but the computer vision researchers stayed in the remaining AT&T Labs, and they couldn't even sell or improve the product without having the rights to the patents.\n",
      "\n",
      "LeCunn was an underdog for most of his life, the deep learning explosion only started happening around 2012 with AlexNet, when conv nets started getting all the attention.\n",
      "Here's a sneak peek of /r/KenM using the [top posts](https://np.reddit.com/r/KenM/top/?sort=top&t=year) of the year!\n",
      "\n",
      "\\#1: [KenM on billionaires](https://i.redd.it/tl38stlg70g41.jpg) | [164 comments](https://np.reddit.com/r/KenM/comments/f1j7a9/kenm_on_billionaires/)  \n",
      "\\#2: [Ken M on conspiracy theorists](https://i.redd.it/7inbjzicewo41.jpg) | [88 comments](https://np.reddit.com/r/KenM/comments/fp01kq/ken_m_on_conspiracy_theorists/)  \n",
      "\\#3: [One of my favorites over the years.](https://i.imgur.com/vjhwVXg.jpg) | [139 comments](https://np.reddit.com/r/KenM/comments/hmhwzo/one_of_my_favorites_over_the_years/)\n",
      "\n",
      "----\n",
      "^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/joo7mb/blacklist_viii/)\n",
      "Automation can be a very secretive thing and very grey so I can't talk about projects or the details.\n",
      "big if true\n",
      "Yup, that exact sentence speaks about normal fonts, I referred to.\n",
      ">  However, up until 2012 they weren't very useful for most applications.\n",
      "\n",
      "At that time, they were already in use by every call center and bank on earth, were running in every copy of Windows, MacOS, and Android, had dominated speech to text for almost 20 years, et cetera.\n",
      "\n",
      "Between Windows and MacOS, they were in over 50% of US homes.\n",
      "\n",
      "For color, ***The US phone system started using neural networks for de-noising in 1959, bringing their use to almost 200 million people***.\n",
      "\n",
      ".\n",
      "\n",
      "> 2012 marked a transition though, as we then had the hardware support to efficiently train much larger models.\n",
      "\n",
      "Respectfully, this is just kind of not true.\n",
      "I'm sorry, I'm not watching a 50 minute video to try to figure out why you believe that one of the world's largest intellectual pursuits was obscure or overlooked until an image recognition contest.\n",
      "\n",
      "My expectation is that whatever the video actually said was misunderstood.  Have a timestamp?\n",
      "â€œLack of necessity is a great blockerâ€ - Iâ€™m stealing that\n",
      "\\>No, he was an employee at Bell Labs, the product and patents belonged to Bell Labs.\n",
      "\n",
      "I would just like to point out that in other countries (e.g. Germany, France Japan), inventors of a patent are entitled to a percentage of the revenue that this invention generates.\n",
      "\n",
      "This is not the case in the US, though.\n",
      "Oh yes, sorry. I think it's not a single set font, but at least several. But I also think you're right and this was made for printed fonts, so \"normal\" might mean \"very common fonts\".\n",
      "I'm not saying they weren't useful. They clearly had use cases as you mentioned. \n",
      "\n",
      "But if you look through ML papers you can clearly see an increase in interest after 2012. And in my experience as an ML engineer, there was a similar increase in interest on the business side after 2012 as well (though often lagging behind SOTA by a few years)\n",
      "He says it in the beginning of the video.\n",
      "I figured it's probably all printed fonts that aren't cursive or Comic Sans. You're definitely right that it's multiple, I think the limitation is just on the type of font.\n",
      "> But if you look through ML papers you can clearly see an increase in interest after 2012.\n",
      "\n",
      "ML papers still haven't caught up to their 1950s heyday, either in volume or in range.  As an issue of measurable fact, we continue to reel not just from the second AI winter, but also from the first.\n",
      "\n",
      "No, sir, today we are not inventing Lisp or Symbolics.  \n",
      "\n",
      "You keep saying SOTA.  This suggests to me that you're an internet fan.  Actual academics and actual industry people don't say that.\n",
      "\n",
      "Please have a good day.\n",
      "I watched the first three minutes.  I don't see anything supporting your claim, or any related evidence.  A timestamp would provide falsifiability, but you declined.\n",
      "\n",
      "There is ample evidence that these were being used by industry for decades, taught at thousands of universities, being discussed by the United Nations.\n",
      "\n",
      "Anyone who's ever seen Star Trek: TNG or Terminator 2 had seen them in the popular consciousness for decades at this point.\n",
      "\n",
      "Every bank had been using them for check scanning for 20+ years at the described point.\n",
      "\n",
      "There were more than a dozen instances where over a billion dollars was invested at a single time into the \"overlooked and forgotten until an image contest\" field.\n",
      "\n",
      "Please have a nice day.\n",
      "Clearly you haven't read many papers published in the last decade then. For better or worse, the term SOTA does show up in recent deep learning papers.... I've also definitely heard it used in my experience within industry as well. It's not super common, but that's a really weird thing to try to gatekeep on\n",
      "The opening concept is conveyed from 00:00 to 5:22.\n",
      "I'm sorry you keep ignoring the evidence and referring to wide swaths of time that do not seem to say what you claim.\n",
      "\n",
      "Claims are concrete.  If he actually says this, you should be able to give a timestamp.  I can't find it, and doubt your interpretation.\n",
      "\n",
      "Common sense says that even if he does say this, just looking at the contrary evidence would be enough to set him aside.  Mark Z Jacobsen is also a teacher at a prestigious university, y'know?  So is Scott Atlas.\n",
      "\n",
      "If the evidence disagrees with an academic, believe the evidence.  I can't even find the academic saying what you claim, and it seems like you can't either.\n",
      "\n",
      "Please have a good day.\n",
      "I was in the industry before 2012.  I have first hand experience.  I remember it too.  If you will not take it from an MIT professor teaching the topic, then who will you take it from?\n",
      "> I remember it too. If you will not take it from an MIT professor teaching the topic, then who will you take it from?\n",
      "\n",
      "Actual history and evidence are fine, thanks.  I already covered this material:\n",
      "\n",
      "> If the evidence disagrees with an academic, believe the evidence. I can't even find the academic saying what you claim, and it seems like you can't either.\n",
      "\n",
      "In the meantime, ***this MIT professor does not actually say the thing you keep pretending he's saying***.\n",
      "\n",
      "Feel free to look up the two names I just gave.  One is a solar crank, also an honored Stanford professor, with a habit of suing people to silence them from pointing out his mistakes.  The other is Trump's medical mess (similarly Stanford.)\n",
      "\n",
      "Want an MIT professor?  Brian Josephsen is a dual-nobel winning MIT physicist who thinks climate change isn't real and sat in court saying cigarettes don't cause cancer.\n",
      "\n",
      "If I can point to their extensive use in every corner of society, that is sufficient to demonstrate that they were not overlooked or forgotten.\n",
      "\n",
      "***I'm sorry you're clinging to something a professor didn't even say.  However, until you can be specific about where he says it, you don't get to stand on his reputation at all, this way.***  Even if you did find it, the burden of evidence would simply show that he's incorrect.\n",
      "\n",
      ".\n",
      "\n",
      "> I was in the industry before 2012. I have first hand experience.\n",
      "\n",
      "Pressing X.\n",
      "\n",
      ".\n",
      "\n",
      "The reason I keep saying \"please have a good day\" is that I am trying to politely end the conversation\n",
      ">In the meantime, this MIT professor does not actually say the thing you keep pretending he's saying.\n",
      "\n",
      "Here is the actual transcript from the beginning of the video:\n",
      "\n",
      ">PATRICK WINSTON: It was in 2010, yes, that's right. It was in 2010. We were having our annual discussion about what we would dump fro 6034 in order to make room for some other stuff. And we almost killed off neural nets. That might seem strange because our heads are stuffed with neurons. If you open up your skull and pluck them all out, you don't think anymore. So it would seem that neural nets would be a fundamental and unassailable topic.\n",
      "\n",
      ">But many of us felt that the neural models of the day weren't much in the way of faithful models of what actually goes on inside our heads. And besides that, nobody had ever made a neural net that was worth a darn for doing anything. So we almost killed it off. But then we said, well, everybody would feel cheated if they take a course in artificial intelligence, don't learn anything about neural nets, and then they'll go off and invent them themselves. And they'll waste all sorts of time. So we kept the subject in.\n",
      "\n",
      ">Then two years later, Jeff Hinton from the University of Toronto stunned the world with some neural network he had done on recognizing and classifying pictures. And he published a paper from which I am now going to show you a couple of examples. Jeff's neural net, by the way, had 60 million parameters in it. And its purpose was to determine which of 1,000 categories best characterized a picture.\n",
      "\n",
      "And he goes on about the topic.\n",
      "Seems like you're badly misunderstanding his story.  He's talking about the MIT curriculum, not the national industry and consciousness.  No wonder you tried so hard not to be specific.\n",
      "\n",
      "The reason I keep saying \"please have a good day\" is that I am trying to politely end the conversation\n",
      "In 2010 the view on NNs was, \"nobody had ever made a neural net that was worth a darn for doing anything.\"\n",
      "\n",
      "And before you start spouting off single perceptrons and calling them neural networks, keep in mind before 2012 people didn't casually call those neural networks (Where's the network?).  It wasn't until 2012 with the CNN that people started to consider neural networks worth anything.\n",
      "> > > > The reason I keep saying \"please have a good day\" is that I am trying to politely end the conversation\n",
      "> >\n",
      "> > The reason I keep saying \"please have a good day\" is that I am trying to politely end the conversation\n",
      "\n",
      "The reason I keep saying \"please have a good day\" is that I am trying to politely end the conversation\n"
     ]
    }
   ],
   "source": [
    "# teste\n",
    "submission = reddit.submission(\"kuc6tz\")\n",
    "\n",
    "submission.comments.replace_more(limit=None)\n",
    "for comment in submission.comments.list():\n",
    "    print(comment.body)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_list = []\n",
    "\n",
    "for post_id in posts_df['post_id']:\n",
    "    submission = reddit.submission(post_id)\n",
    "\n",
    "    submission.comments.replace_more(limit=None)\n",
    "    for comment in submission.comments.list():\n",
    "        comments_list.append({'post_id': post_id, 'comment':comment.body})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = pd.DataFrame(comments_list)\n",
    "comments_df.to_csv('data/ds_ml_ai_reddit_comments.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
